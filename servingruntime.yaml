apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-transformers
  labels:
    name: triton-transformers
  annotations:
    # maxLoadingConcurrency: "2"
    openshift.io/display-name: "Triton for Transformer Models (PyTorch)"
spec:
  supportedModelFormats:
    - name: keras
      version: "2" # 2.6.0
      autoSelect: true
    - name: onnx
      version: "1" # 1.5.3
      autoSelect: true
    - name: pytorch
      version: "1" # 1.8.0a0+17f8c32
      autoSelect: true
    - name: tensorflow
      version: "1" # 1.15.4
      autoSelect: true
    - name: tensorflow
      version: "2" # 2.3.1
      autoSelect: true
    - name: triton
      version: "2"
      autoSelect: true

  protocolVersions:
    - v2
    - grpc-v2
  multiModel: false

  # grpcEndpoint: "port:8085"
  # grpcDataEndpoint: "port:8001"

  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi
  containers:
    - name: kserve-container
      # Custom Triton server image with torch/transformers dependencies (CUDA 11.8)
      # Version: v2 - PyTorch 2.1.0, Transformers 4.34.1, CUDA 11.8
      image: quay.io/rh_ee_sgleszer/tritonserver-torch:23.05-py3-torch2.1-cu118-v2
      env:
        - name: HF_HOME
          value: /tmp/hf_cache
        - name: TRANSFORMERS_CACHE
          value: /tmp/hf_cache
        - name: TRITON_BACKEND_STUB_TIMEOUT_SECONDS
          value: "600"
        - name: OMP_NUM_THREADS
          value: "4"
      command: [/bin/sh]
      args:
        - -c
        - 'exec tritonserver
          "--model-repository=/mnt/models/"
          "--model-control-mode=poll"
          "--disable-auto-complete-config"
          "--strict-readiness=false"
          "--allow-http=true"
          "--allow-sagemaker=false"
          '
      volumeMounts:
        - name: shm
          mountPath: /dev/shm
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: "5"
          memory: 20Gi
      livenessProbe:
        # the server is listening only on 127.0.0.1, so an httpGet probe sent
        # from the kublet running on the node cannot connect to the server
        # (not even with the Host header or host field)
        # exec a curl call to have the request originate from localhost in the
        # container
        exec:
          command:
            - curl
            - --fail
            - --silent
            - --show-error
            - --max-time
            - "9"
            - http://localhost:8000/v2/health/live
        initialDelaySeconds: 300
        periodSeconds: 30
        timeoutSeconds: 10
      readinessProbe:
        exec:
          command:
            - curl
            - --fail
            - --silent
            - --show-error
            - --max-time
            - "9"
            - http://localhost:8000/v2/health/live
        initialDelaySeconds: 300
        periodSeconds: 30
        timeoutSeconds: 10
  builtInAdapter:
    serverType: triton
    runtimeManagementPort: 8001
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 600000